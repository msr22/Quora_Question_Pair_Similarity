{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define load method\n",
    "def load_dataset(path, feature_cols, target_cols):\n",
    "    dataset = pd.read_csv(path, keep_default_na = False)\n",
    "    \n",
    "    question_dataframe = pd.DataFrame(dataset, columns = feature_cols)\n",
    "    labels = pd.DataFrame(dataset, columns = target_cols)\n",
    "    \n",
    "    question_array = question_dataframe.to_numpy()\n",
    "    labels_array = labels.to_numpy()\n",
    "    \n",
    "    return question_array, labels_array    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset load path\n",
    "base_path = '.'\n",
    "dataset_path =  base_path + '/question-pairs-dataset/questions.csv'\n",
    "\n",
    "#dataset feature and target columns\n",
    "features = ['question1', 'question2']\n",
    "target = ['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "x, y = load_dataset(dataset_path, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Pair: (404351, 2)\n",
      "Question Similarity: (404351, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check dataset shape in feature(x) and target(y)\n",
    "print('Question Pair:',x.shape)\n",
    "print('Question Similarity:',y.shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define method to split dataset\n",
    "def split_dataset(x, y, train_ratio):\n",
    "    assert x.shape[0] == y.shape[0], 'different num rows in feature and target'\n",
    "    \n",
    "    num_rows_split = int(x.shape[0] * train_ratio)\n",
    "    \n",
    "    x_train = x[:num_rows_split]\n",
    "    x_test = x[num_rows_split:]\n",
    "    \n",
    "    y_train = y[:num_rows_split]\n",
    "    y_test = y[num_rows_split:]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define test_train split ratio\n",
    "split_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split dataset\n",
    "x_train, y_train, x_test, y_test = split_dataset(x, y, split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train & y_train: (323480, 2)  &  (323480, 1)\n",
      "x_test & y_test: (80871, 2)  &  (80871, 1)\n"
     ]
    }
   ],
   "source": [
    "#verify shape of train and test splits\n",
    "print('x_train & y_train:',x_train.shape, ' & ', y_train.shape)\n",
    "print('x_test & y_test:',x_test.shape, ' & ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing the Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4.1 Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define pretrained model loader\n",
    "def read_pretrained_model(glove_path):\n",
    "    \n",
    "    with open(glove_path, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            \n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            \n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype = np.float32)\n",
    "            \n",
    "        \n",
    "        i = 1\n",
    "        word_to_index = {}\n",
    "        index_to_word = {}\n",
    "        \n",
    "        for w in sorted(words):\n",
    "            \n",
    "            word_to_index[w] = i\n",
    "            index_to_word[i] = w\n",
    "            \n",
    "            i = i + 1\n",
    "        \n",
    "        return word_to_index, index_to_word, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove vector path\n",
    "glove_path = './glove_model/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading glove vector\n",
    "word_to_index, index_to_word, word_to_vec_map = read_pretrained_model(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of  security  in vocabulary is  323224\n",
      "Word of  323224  in vocabulary is  security\n",
      "Vector of  security  in vocabulary is\n",
      "  [-1.8194e-01  1.3781e-01  3.9300e-02 -2.0317e-01  3.7706e-01 -1.5607e-02\n",
      "  1.1759e-01  1.0152e+00 -1.4270e-01 -2.7698e+00  1.1016e-01  1.8549e-02\n",
      "  4.4524e-01 -3.3648e-01 -5.3132e-01  3.3976e-01  2.6947e-01  4.7103e-02\n",
      " -3.3890e-01  2.1926e-03  2.1345e-01 -2.6047e-01  1.9542e-01 -6.3285e-01\n",
      " -5.0128e-01  4.4029e-01  2.2583e-02  4.8802e-01 -2.8056e-01  4.5661e-01\n",
      "  3.9620e-01 -3.1185e-01 -5.3025e-01  4.5025e-01 -2.9634e-02 -3.3689e-01\n",
      " -1.3732e-01  1.6462e-01 -5.4839e-01 -6.0662e-01 -2.2178e-02 -6.3760e-02\n",
      "  3.3564e-01  6.5757e-02 -4.0945e-01  1.6017e-01 -3.6239e-01  1.3025e-01\n",
      " -7.0111e-02  1.2693e-01 -2.9299e-01 -5.9315e-02 -4.3517e-01 -2.9694e-02\n",
      "  4.2646e-01 -2.3699e-01 -3.3968e-01  1.0792e-01 -1.7100e-01 -4.3971e-01\n",
      "  2.6197e-01  2.8372e-01 -2.4098e-01 -4.8820e-03 -3.1007e-01 -1.7094e-01\n",
      "  2.5328e-01  2.9205e-01  4.8540e-01 -4.5817e-01 -9.7190e-02 -1.8966e-01\n",
      "  9.6172e-02 -4.9902e-01  2.9816e-02  5.9622e-01 -3.9641e-01  1.0016e-01\n",
      "  9.5756e-02 -2.2084e-01  1.8251e-01  8.2379e-02  1.7511e-01  1.1182e-01\n",
      "  8.5762e-01 -1.1009e-01 -4.5506e-01  6.0843e-02 -2.1475e-01 -5.2175e-01\n",
      " -9.0916e-01  3.6757e-01 -2.4493e-01  6.5764e-01 -2.9359e-01  1.7591e-01\n",
      " -5.3310e-01 -1.6142e-01 -1.2855e-01 -5.0400e-01 -1.6516e-01  2.5640e-01\n",
      "  2.7836e-01 -4.7419e-01  2.1308e-01  9.5696e-02 -2.0492e-01  3.2057e-01\n",
      " -6.3579e-01 -5.9111e-01  7.9475e-02 -2.1228e-01  6.8380e-02 -3.0342e-01\n",
      "  2.1545e-01  6.0828e-01  3.6816e-01 -1.7394e-01  7.2822e-02 -4.6542e-01\n",
      "  5.7921e-02 -2.3608e-02  1.0598e-01 -1.0941e-01  2.4369e-01 -7.2569e-01\n",
      "  2.8296e-01  2.2465e-02  2.1543e-01 -4.6186e-02  1.0857e-01  4.8152e-02\n",
      " -5.2460e-02 -3.4529e-01 -3.5771e-01 -2.2547e-01  1.3642e-01 -2.5027e-01\n",
      "  6.4070e-01  1.0573e-01 -5.9183e-01  3.2966e-01  8.4919e-02 -1.3607e-02\n",
      " -6.8741e-01  1.1908e-01  4.7863e-01 -1.6985e-01 -5.7571e-01 -1.8719e-01\n",
      "  7.3935e-01 -2.6841e-01  7.6738e-01  5.0109e-02 -1.2604e-01 -5.4097e-01\n",
      " -2.8333e-01  8.8998e-02  3.3764e-01 -3.3712e-01 -4.0964e-02  1.0028e-01\n",
      "  3.1361e-03 -4.1629e-01 -2.2565e-01 -2.2907e-01 -5.7340e-02  6.1719e-01\n",
      "  4.3920e-02  1.1945e-01  3.2709e-01 -7.8180e-02 -4.6106e-02 -1.4723e-01\n",
      " -1.7727e-01 -3.2632e-01  1.5827e-01 -1.1742e-01  2.8840e-01 -2.1000e-01\n",
      " -2.0098e-01 -4.4657e-01 -5.0022e-02  1.3641e-01 -4.4214e-01  1.8898e-01\n",
      " -2.3351e-01 -1.8250e-02 -5.3742e-02  3.6228e-01  3.5991e-01 -5.3417e-01\n",
      " -1.3493e-01 -1.5659e-01 -4.5005e-01  1.8124e-01  1.9037e-01  1.7190e-02\n",
      "  7.4946e-01  4.7709e-01  2.9092e-01  3.0915e-01  2.8285e-01  2.6425e-01\n",
      "  7.5144e-02 -4.6746e-01  4.2963e-01 -8.1316e-01  2.9849e-01 -3.4027e-01\n",
      "  6.1068e-01  8.4481e-03  1.0376e-01  2.4671e-01  5.4721e-01 -1.0888e-01\n",
      " -7.3881e-02  1.3583e-01  2.8289e-01  5.7272e-01  4.4748e-01 -9.4922e-02\n",
      "  5.3528e-01  7.0386e-01  3.3146e-01 -4.1432e-01  1.3248e-01 -1.9854e-01\n",
      " -1.1498e-01  8.4157e-02 -6.7350e-02  1.8843e-03  3.1002e-01 -2.9461e-01\n",
      "  1.8640e-01  1.4053e-01 -1.7386e-01 -4.0004e-02  5.3634e-01  1.9556e-01\n",
      "  1.8632e-01  3.1827e-01  1.0490e+00 -1.0645e-01  3.3769e-01 -6.2312e-01\n",
      "  1.8416e-01 -9.0011e-01  9.0576e-02 -1.7003e-01 -1.8465e-01  4.3653e-01\n",
      "  8.7866e-01  7.2475e-02  2.2944e-01 -1.3400e-01 -1.8227e-02  2.9584e-01\n",
      "  1.4443e-01  5.3766e-01  5.1946e-02  9.2206e-02  2.1258e-01  1.8864e-01\n",
      " -4.0326e-03 -1.4993e-01 -2.2239e-01  2.5216e-01  9.3789e-02 -1.1946e-01\n",
      "  3.1405e-01  2.6964e-01 -6.7427e-01  3.8141e-01 -1.5643e-01 -2.7748e-01\n",
      " -2.7396e+00  4.6732e-01  9.1589e-01  1.4031e-01  5.0485e-01 -9.6371e-02\n",
      "  3.8589e-01  2.4439e-01  2.6059e-01  1.1001e-02  4.3584e-01  2.1841e-01\n",
      "  3.1128e-01 -2.2588e-01  2.9982e-01 -5.0868e-01 -4.0496e-01 -1.2137e-01\n",
      " -8.8518e-03  1.0069e+00 -2.3767e-01  2.8653e-02 -1.2082e+00 -1.8522e-02]\n"
     ]
    }
   ],
   "source": [
    "#Test glove vectors\n",
    "word = 'security'\n",
    "index = 323224\n",
    "\n",
    "print('Index of ',word,' in vocabulary is ', word_to_index[word])\n",
    "print('Word of ',index,' in vocabulary is ', index_to_word[index])\n",
    "print('Vector of ',word,' in vocabulary is\\n ', word_to_vec_map[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Convert Input Text-to-Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load spacy model\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define method for sentence into indices \n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    num_sentences = X.shape[1]\n",
    "        \n",
    "    X_indices = np.zeros((m, num_sentences, max_len))\n",
    "    \n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        for k in range(num_sentences):\n",
    "                        \n",
    "            sentence_words = [str(word).lower() for word in list(nlp(X[i][k]))]\n",
    "                        \n",
    "            j = 0\n",
    "            \n",
    "            for w in sentence_words:\n",
    "                X_indices[i, k, j] = word_to_index.get(w, 0)\n",
    "                j += 1\n",
    "\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_index_train = sentences_to_indices(x_train, word_to_index, max_len=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_index_test = sentences_to_indices(x_test, word_to_index, max_len=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(323480, 2, 500)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_index_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define pre-trained embedding layer\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "        \n",
    "    vocab_len = len(word_to_index) + 1                  \n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]     \n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_len, emb_dim, trainable=False, mask_zero= True)\n",
    "   \n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define text-encoding model\n",
    "class EncodingModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EncodingModel, self).__init__()\n",
    "        self.embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "        \n",
    "        self.lstm_layer1 = tf.keras.layers.LSTM(128, return_sequences = True)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "        \n",
    "        self.lstm_layer2 = tf.keras.layers.LSTM(128, return_sequences = False)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(128)\n",
    "        \n",
    "        self.dense_combined = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.classification = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding_layer(inputs[:, 0, :])            #For Question1\n",
    "        x = self.lstm_layer1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lstm_layer2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        y = self.embedding_layer(inputs[:, 1, :])            #For Question2\n",
    "        y = self.lstm_layer1(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.lstm_layer2(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.dense(y)\n",
    "        \n",
    "        z = tf.math.abs(tf.subtract(x, y))\n",
    "        z = self.dense_combined(z)\n",
    "        z = self.classification(z)\n",
    "        \n",
    "               \n",
    "                \n",
    "        return z     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    encoding_model = EncodingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_model.compile(loss = 'binary_crossentropy', optimizer = tf.train.AdamOptimizer(0.0025), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define checkpoint path\n",
    "checkpoint_path = './model_files/weights.{epoch:02d}-{val_loss:.2f}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True,\n",
    "    # Save weights, every epoch\n",
    "    period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323480 samples, validate on 80871 samples\n",
      "Epoch 1/6\n",
      "323328/323480 [============================>.] - ETA: 2s - loss: 0.4596 - acc: 0.7754\n",
      "Epoch 00001: saving model to ./model_files/weights.01-0.52.ckpt\n",
      "WARNING:tensorflow:From /home/saurabh/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "323480/323480 [==============================] - 6173s 19ms/sample - loss: 0.4596 - acc: 0.7754 - val_loss: 0.5181 - val_acc: 0.7539\n",
      "Epoch 2/6\n",
      "323328/323480 [============================>.] - ETA: 2s - loss: 0.4309 - acc: 0.7928\n",
      "Epoch 00002: saving model to ./model_files/weights.02-0.51.ckpt\n",
      "323480/323480 [==============================] - 6039s 19ms/sample - loss: 0.4309 - acc: 0.7928 - val_loss: 0.5051 - val_acc: 0.7705\n",
      "Epoch 3/6\n",
      "323328/323480 [============================>.] - ETA: 2s - loss: 0.4074 - acc: 0.8067\n",
      "Epoch 00003: saving model to ./model_files/weights.03-0.49.ckpt\n",
      "323480/323480 [==============================] - 6032s 19ms/sample - loss: 0.4073 - acc: 0.8068 - val_loss: 0.4939 - val_acc: 0.7781\n",
      "Epoch 4/6\n",
      "323328/323480 [============================>.] - ETA: 2s - loss: 0.3884 - acc: 0.8188\n",
      "Epoch 00004: saving model to ./model_files/weights.04-0.51.ckpt\n",
      "323480/323480 [==============================] - 6036s 19ms/sample - loss: 0.3884 - acc: 0.8188 - val_loss: 0.5108 - val_acc: 0.7685\n",
      "Epoch 5/6\n",
      "323328/323480 [============================>.] - ETA: 2s - loss: 0.3712 - acc: 0.8283\n",
      "Epoch 00005: saving model to ./model_files/weights.05-0.52.ckpt\n",
      "323480/323480 [==============================] - 6048s 19ms/sample - loss: 0.3712 - acc: 0.8283 - val_loss: 0.5158 - val_acc: 0.7741\n",
      "Epoch 6/6\n",
      "323328/323480 [============================>.] - ETA: 3s - loss: 0.3557 - acc: 0.8377\n",
      "Epoch 00006: saving model to ./model_files/weights.06-0.49.ckpt\n",
      "323480/323480 [==============================] - 8625s 27ms/sample - loss: 0.3557 - acc: 0.8377 - val_loss: 0.4868 - val_acc: 0.7844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1022271f98>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_model.fit(x_index_train, y_train, batch_size=256, epochs=6, verbose=1, callbacks= [cp_callback], validation_data= (x_index_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      multiple                  120000300 \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  66048     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  513       \n",
      "=================================================================\n",
      "Total params: 120,434,605\n",
      "Trainable params: 434,305\n",
      "Non-trainable params: 120,000,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction on Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = encoding_model.predict(x_index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_labels = (y_predict > 0.55).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Classification Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80871, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = metrics.confusion_matrix(y_test, y_predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = metrics.f1_score(y_test, y_predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39788, 11987],\n",
       "       [ 4673, 24423]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.745672152169267"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
